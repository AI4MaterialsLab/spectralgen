{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7eddf1",
   "metadata": {},
   "source": [
    "# Spectralgen Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53142cc",
   "metadata": {},
   "source": [
    "Code related to the work \"J.J. García-Esteban, J.C. Cuevas, J. Bravo-Abad, \"Generative adversarial networks for data-scarce spectral applications in the physical sciences”, submitted for publication (2023).\"\n",
    "\n",
    "Contains:\n",
    "\n",
    "- code used to create and train the networks described. \n",
    "\n",
    "- code used to create some of the graphs in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea4a37",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c7b95",
   "metadata": {},
   "source": [
    "Run this code to import the relevant libraries, load and prepare the datasets and define the evaluation metrics used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47290eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "matplotlib.rcParams.update({'font.size': 30})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "# Total amount of data: 6561 examples\n",
    "\n",
    "label_path = '/Users/usuario/Desktop/GAN_multilayered/labels8metal.csv'\n",
    "data_path = '/Users/usuario/Desktop/GAN_multilayered/data8metal.csv'\n",
    "\n",
    "dataindex_raw = np.genfromtxt(label_path,dtype=\"float32\") # RAW indices\n",
    "dataindex = dataindex_raw[:,[1,2,3,4,5,6,7,8]]\n",
    "datafile = np.genfromtxt(data_path,dtype=\"float32\") # RAW spectra\n",
    "\n",
    "# Random_index\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "random_index = np.linspace(0,dataindex.shape[0]-1,dataindex.shape[0])\n",
    "np.random.shuffle(random_index)\n",
    "      \n",
    "# Normalization\n",
    "\n",
    "input_mean = np.mean(np.log10(dataindex),axis=0)\n",
    "input_std = np.std(np.log10(dataindex),axis=0)\n",
    "\n",
    "data_mean = np.mean(np.log10(datafile),axis=0)\n",
    "data_std = np.std(np.log10(datafile),axis=0)\n",
    "\n",
    "# Preallocate the sets\n",
    "\n",
    "# Target data\n",
    "\n",
    "y_target = np.zeros([datafile.shape[0],datafile.shape[1]]) \n",
    "y_target_rand = np.zeros([datafile.shape[0],datafile.shape[1]])\n",
    "\n",
    "# Input data\n",
    "\n",
    "y_in = np.zeros([dataindex.shape[0],dataindex.shape[1]])\n",
    "y_in_rand = np.zeros([dataindex.shape[0],dataindex.shape[1]])\n",
    "\n",
    "# Populate the sets\n",
    "\n",
    "for i in range(dataindex.shape[0]):\n",
    "\n",
    "    index = int(random_index[i])\n",
    "    \n",
    "    y_in[i,:] = (np.log10(dataindex[i,:])-input_mean)/input_std\n",
    "    y_target[i,:] = (np.log10(datafile[i,:])-data_mean)/data_std\n",
    "    \n",
    "    y_in_rand[i,:] = (np.log10(dataindex[index,:])-input_mean)/input_std\n",
    "    y_target_rand[i,:] = (np.log10(datafile[index,:])-data_mean)/data_std\n",
    "\n",
    "# Create the train and val sets\n",
    "\n",
    "val = 0.2 # Validation fraction [0,1]\n",
    "\n",
    "train_index = np.linspace(0,np.int((1-val)*datafile.shape[0]-1),np.int((1-val)*datafile.shape[0])).astype(int)\n",
    "val_index = np.linspace(np.int((1-val)*datafile.shape[0]),datafile.shape[0]-1,np.int(val*datafile.shape[0])+1).astype(int)\n",
    "\n",
    "y_in_train = y_in_rand[train_index,:]\n",
    "y_in_val = y_in_rand[val_index,:]\n",
    "\n",
    "y_target_train = y_target_rand[train_index,:]\n",
    "y_target_val = y_target_rand[val_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27439dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "# FFNN\n",
    "\n",
    "def mean_relative_abs_error_pointwise(net, inputs, target):\n",
    "    \n",
    "    Npoints = target.shape[1]\n",
    "    Nexamples = target.shape[0]\n",
    "    \n",
    "    error = np.zeros([1,1])\n",
    "    \n",
    "    for i in range(Nexamples):\n",
    "        \n",
    "        fake = np.reshape(net.predict_on_batch(np.reshape(inputs[i,:],(1,inputs.shape[1]))),(Npoints,))\n",
    "        \n",
    "        faked = 10**(fake*data_std+data_mean)\n",
    "        realed = 10**(target[i,:]*data_std+data_mean)\n",
    "        \n",
    "        for j in range(Npoints):\n",
    "            \n",
    "            error = error + np.abs((faked[j]-realed[j])/realed[j])\n",
    "    \n",
    "    return error/(Npoints*Nexamples)*100\n",
    "\n",
    "def integral_relative_error(net, inputs, target):\n",
    "    \n",
    "    Npoints = target.shape[1]\n",
    "    Nexamples = target.shape[0]\n",
    "    \n",
    "    error = np.zeros([1,1])\n",
    "    \n",
    "    for i in range(Nexamples):\n",
    "        \n",
    "        fake = np.reshape(net.predict_on_batch(np.reshape(inputs[i,:],(1,inputs.shape[1]))),(Npoints,))\n",
    "        \n",
    "        faked = 10**(fake*data_std+data_mean)\n",
    "        realed = 10**(target[i,:]*data_std+data_mean)\n",
    "        \n",
    "        error = error + np.abs((np.trapz(faked)-np.trapz(realed))/np.trapz(realed))\n",
    "        \n",
    "    return error/Nexamples*100\n",
    "\n",
    "# GAN, CGAN, CWGAN\n",
    "\n",
    "def mean_relative_abs_error_pointwise_gan(net, inputs, target):\n",
    "    \n",
    "    Npoints = target.shape[1]\n",
    "    Nexamples = target.shape[0]\n",
    "    \n",
    "    error = np.zeros([1,1])\n",
    "\n",
    "    fake = net([inputs]).numpy()\n",
    "    \n",
    "    for i in range(Nexamples):\n",
    "        \n",
    "        faked = 10**(fake[i,:]*data_std+data_mean)\n",
    "        realed = 10**(target[i,:]*data_std+data_mean)\n",
    "        \n",
    "        for j in range(Npoints):\n",
    "            \n",
    "            error = error + np.abs((faked[j]-realed[j])/realed[j])\n",
    "    \n",
    "    return error/(Npoints*Nexamples)*100\n",
    "\n",
    "def integral_relative_error_gan(net, inputs, target):\n",
    "    \n",
    "    Npoints = target.shape[1]\n",
    "    Nexamples = target.shape[0]\n",
    "    \n",
    "    error = np.zeros([1,1])\n",
    "    \n",
    "    fake = net([inputs]).numpy()\n",
    "    \n",
    "    for i in range(Nexamples):\n",
    "        \n",
    "        faked = 10**(fake[i,:]*data_std+data_mean)\n",
    "        realed = 10**(target[i,:]*data_std+data_mean)\n",
    "                \n",
    "        error = error + np.abs((np.trapz(faked)-np.trapz(realed))/np.trapz(realed))\n",
    "        \n",
    "    return error/Nexamples*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b594cd",
   "metadata": {},
   "source": [
    "## Train a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c45a2",
   "metadata": {},
   "source": [
    "Run this code to create and train a network like the ones used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6764f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FFNN\n",
    "\n",
    "# Defining the network\n",
    "\n",
    "net_o=Sequential()\n",
    "net_o.add(Dense(200, input_shape=(8,), activation = 'selu'))\n",
    "net_o.add(Dense(200, activation = 'selu'))\n",
    "net_o.add(Dense(200, activation = 'selu'))\n",
    "net_o.add(Dense(200, activation = 'selu'))\n",
    "net_o.add(Dense(200, activation = 'selu'))\n",
    "net_o.add(Dense(datafile.shape[1]))\n",
    "\n",
    "# Compiling the network\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0003)\n",
    "net_o.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69600bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a FFNN\n",
    "\n",
    "batchsize = dataindex.shape[0]\n",
    "batches = 20000\n",
    "\n",
    "net2_o = net_o.fit(y_in_rand,y_target_rand,batch_size=batchsize, validation_split = val, epochs=batches, verbose = 0)\n",
    "loss = net2_o.history['loss'] #recording of loss\n",
    "val_loss = net2_o.history['val_loss'] #recording of val_loss\n",
    "\n",
    "# Evaluate the FFNN\n",
    "\n",
    "print(mean_relative_abs_error_pointwise(net_o, y_in_train, y_target_train))\n",
    "print(mean_relative_abs_error_pointwise(net_o, y_in_val, y_target_val))\n",
    "print(integral_relative_error(net_o, y_in_train, y_target_train))\n",
    "print(integral_relative_error(net_o, y_in_val, y_target_val))\n",
    "\n",
    "# Plot the evolution of the loss\n",
    "\n",
    "plt.plot(loss,color='blue')\n",
    "plt.plot(val_loss,color='black')\n",
    "plt.title('Train and val loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CWGAN\n",
    "\n",
    "# Step 1: generator\n",
    "\n",
    "input_label = Input(shape = (8,))\n",
    "\n",
    "gen = layers.Dense(50, activation = 'selu')(input_label)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(100, activation = 'selu')(gen)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(150, activation = 'selu')(gen)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(200, activation = 'selu')(gen)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(y_target_rand.shape[1], activation = 'linear')(gen)\n",
    "\n",
    "genr = Model(input_label,gen)\n",
    "\n",
    "# Step 2: discriminator\n",
    "\n",
    "input_discr_spect = Input(shape = (y_target_rand.shape[1]))\n",
    "input_discr_label = Input(shape = (8,))\n",
    "\n",
    "discr_spect = layers.Dense(150, activation = 'selu')(input_discr_spect)\n",
    "discr_label = layers.Dense(50, activation = 'selu')(input_discr_label)\n",
    "\n",
    "intermediate = layers.concatenate([discr_spect,discr_label])\n",
    "\n",
    "discr = layers.Dense(100, activation = 'selu')(intermediate)\n",
    "discr = layers.Dense(50, activation = 'selu')(discr)\n",
    "discr = layers.Dense(1, activation = 'linear')(discr)\n",
    "\n",
    "discrr = Model([input_discr_spect,input_discr_label],discr)\n",
    "\n",
    "# Step 3: Optimizers\n",
    "\n",
    "discr_opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "gen_opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "# Step 4: loss functions (and gradient penalty)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = tf.reduce_mean(real)\n",
    "    fake_loss = tf.reduce_mean(fake)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "def generator_loss(fake, pred):\n",
    "    return -tf.reduce_mean(fake) + tf.reduce_mean(tf.math.abs(pred-tf.convert_to_tensor(y_target_train,dtype=tf.float32)))  \n",
    "\n",
    "def gradient_penalty(real, fake, label):\n",
    "    \"\"\" Calculates the gradient penalty. This loss is calculated on an interpolated image \n",
    "    and added to the discriminator loss.\n",
    "    \"\"\"\n",
    "    # Get the interpolated example\n",
    "    alpha = tf.random.normal([y_target_train.shape[0], 1], 0, 1)\n",
    "    diff = fake - real\n",
    "    interp = real + alpha * diff\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        pred = discrr([interp, label], training=True)\n",
    "\n",
    "    grad = gp_tape.gradient(pred, [interp])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    \n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CWGAN\n",
    "\n",
    "# Preallocate and define constants\n",
    "\n",
    "n_epochs = 50000 # Training epochs\n",
    "d_steps = 5      # Training advantage for the critic\n",
    "gp_weight = 10   # Weight of the Gradient Penalty\n",
    "\n",
    "y_in_latent = tf.convert_to_tensor(y_in_train,dtype=tf.float32)\n",
    "\n",
    "# Train the network \n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # Step 1: train the discriminator, d_steps times per gen step\n",
    "    \n",
    "    for j in range(d_steps):\n",
    "                \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Generate fake results \n",
    "            fake = genr(y_in_latent, training=True)\n",
    "            \n",
    "            # Get the logits for the fake results\n",
    "            fake_logits = discrr([fake,y_in_latent], training=True)\n",
    "            \n",
    "            # Get the logits for the real data\n",
    "            real_logits = discrr([tf.convert_to_tensor(y_target_train,dtype=tf.float32),y_in_latent], training=True)\n",
    "            \n",
    "            # Calculate the discriminator loss using the fake and real logits\n",
    "            d_cost = discriminator_loss(real_logits, fake_logits)\n",
    "            \n",
    "            # Calculate the gradient penalty\n",
    "            gp = gradient_penalty(tf.convert_to_tensor(y_target_train,dtype=tf.float32), fake, y_in_latent)\n",
    "            \n",
    "            # Add the gp to the original loss\n",
    "            d_loss = d_cost + gp * gp_weight\n",
    "        \n",
    "        # Get the gradients \n",
    "        d_gradient = tape.gradient(d_loss, discrr.trainable_variables)\n",
    "        \n",
    "        # Update the weights \n",
    "        discr_opt.apply_gradients(zip(d_gradient, discrr.trainable_variables))\n",
    "        \n",
    "    # Step 2: train the generator\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Generate fake results \n",
    "        generated_curves = genr(y_in_latent, training=True)\n",
    "        \n",
    "        # Get the discriminator logits for fake results\n",
    "        gen_logits = discrr([generated_curves,y_in_latent], training=True)\n",
    "        \n",
    "        # Calculate the generator loss\n",
    "        g_loss = generator_loss(gen_logits, generated_curves)\n",
    "        \n",
    "    # Record the values of the losses\n",
    "    record1[i] = record11(gen_logits, generated_curves)\n",
    "    record2[i] = record22(gen_logits, generated_curves)\n",
    "        \n",
    "    # Get the gradients \n",
    "    gen_gradient = tape.gradient(g_loss, genr.trainable_variables)\n",
    "    \n",
    "    # Update the weights \n",
    "    gen_opt.apply_gradients(zip(gen_gradient, genr.trainable_variables))\n",
    "    \n",
    "# Evaluate the CWGAN\n",
    "\n",
    "print(mean_relative_abs_error_pointwise_gan(genr, y_in_train, y_target_train))\n",
    "print(mean_relative_abs_error_pointwise_gan(genr, y_in_val, y_target_val))\n",
    "print(integral_relative_error_gan(genr, y_in_train, y_target_train))\n",
    "print(integral_relative_error_gan(genr, y_in_val, y_target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb41ab8",
   "metadata": {},
   "source": [
    "## Evaluate a network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44420a0a",
   "metadata": {},
   "source": [
    "Run this code to evaluate a pre-trained network (also used in next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load networks\n",
    "\n",
    "gen_CGAN = models.load_model('GAN_8metal_020_CGAN.h5')\n",
    "gen_CWGAN = models.load_model('GAN_8metal_020_v3g.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All evaluation metrics\n",
    "\n",
    "gen_trial = gen_CWGAN\n",
    "\n",
    "print(mean_relative_abs_error_pointwise_gan(gen_trial, y_in_train, y_target_train))\n",
    "print(mean_relative_abs_error_pointwise_gan(gen_trial, y_in_val, y_target_val))\n",
    "print(integral_relative_error_gan(gen_trial, y_in_train, y_target_train))\n",
    "print(integral_relative_error_gan(gen_trial, y_in_val, y_target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7f9a0",
   "metadata": {},
   "source": [
    "## First epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ef70a",
   "metadata": {},
   "source": [
    "Run this code to generate a figure similar to figure 5, varies slightly each training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the first N epochs of a CWGAN\n",
    "\n",
    "N = 20000 # Train for the N first steps \n",
    "\n",
    "# Preallocate and define constants\n",
    "\n",
    "record1 = np.zeros(N,) # Store results\n",
    "record2 = np.zeros(N,)\n",
    "\n",
    "n_epochs = N # Training epochs\n",
    "d_steps = 5     # Training advantage for the critic\n",
    "gp_weight = 10  # Weight of the Gradient Penalty\n",
    "\n",
    "y_in_latent = tf.convert_to_tensor(y_in_train,dtype=tf.float32)\n",
    "\n",
    "# Step 1: generator\n",
    "\n",
    "input_label = Input(shape = (8,))\n",
    "\n",
    "gen = layers.Dense(50, activation = 'selu')(input_label)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(100, activation = 'selu')(gen)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(150, activation = 'selu')(gen)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(200, activation = 'selu')(gen)\n",
    "gen = layers.Dropout(0.2)(gen)\n",
    "gen = layers.Dense(y_target_rand.shape[1], activation = 'linear')(gen)\n",
    "\n",
    "genr = Model(input_label,gen)\n",
    "\n",
    "# Step 2: discriminator\n",
    "\n",
    "input_discr_spect = Input(shape = (y_target_rand.shape[1]))\n",
    "input_discr_label = Input(shape = (8,))\n",
    "\n",
    "discr_spect = layers.Dense(150, activation = 'selu')(input_discr_spect)\n",
    "discr_label = layers.Dense(50, activation = 'selu')(input_discr_label)\n",
    "\n",
    "intermediate = layers.concatenate([discr_spect,discr_label])\n",
    "\n",
    "discr = layers.Dense(100, activation = 'selu')(intermediate)\n",
    "discr = layers.Dense(50, activation = 'selu')(discr)\n",
    "discr = layers.Dense(1, activation = 'linear')(discr)\n",
    "\n",
    "discrr = Model([input_discr_spect,input_discr_label],discr)\n",
    "\n",
    "# Step 3: Optimizers\n",
    "\n",
    "discr_opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "gen_opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "# Step 4: loss functions (and gradient penalty)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = tf.reduce_mean(real)\n",
    "    fake_loss = tf.reduce_mean(fake)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "def generator_loss(fake, pred):\n",
    "    return -tf.reduce_mean(fake) + tf.reduce_mean(tf.math.abs(pred-tf.convert_to_tensor(y_target_train,dtype=tf.float32)))\n",
    "\n",
    "def record11(fake, pred):\n",
    "    return -tf.reduce_mean(fake) \n",
    "\n",
    "def record22(fake, pred):\n",
    "    return tf.reduce_mean(tf.math.abs(pred-tf.convert_to_tensor(y_target_train,dtype=tf.float32)))  \n",
    "\n",
    "def gradient_penalty(real, fake, label):\n",
    "    \"\"\" Calculates the gradient penalty. This loss is calculated on an interpolated image \n",
    "    and added to the discriminator loss.\n",
    "    \"\"\"\n",
    "    # Get the interpolated example\n",
    "    alpha = tf.random.normal([y_target_train.shape[0], 1], 0, 1)\n",
    "    diff = fake - real\n",
    "    interp = real + alpha * diff\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        pred = discrr([interp, label], training=True)\n",
    "\n",
    "    grad = gp_tape.gradient(pred, [interp])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    \n",
    "    return gp\n",
    "\n",
    "# Train the network \n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # Step 1: train the discriminator, d_steps times per gen step\n",
    "    \n",
    "    for j in range(d_steps):\n",
    "                \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Generate fake results \n",
    "            fake = genr(y_in_latent, training=True)\n",
    "            \n",
    "            # Get the logits for the fake results\n",
    "            fake_logits = discrr([fake,y_in_latent], training=True)\n",
    "            \n",
    "            # Get the logits for the real data\n",
    "            real_logits = discrr([tf.convert_to_tensor(y_target_train,dtype=tf.float32),y_in_latent], training=True)\n",
    "            \n",
    "            # Calculate the discriminator loss using the fake and real logits\n",
    "            d_cost = discriminator_loss(real_logits, fake_logits)\n",
    "            \n",
    "            # Calculate the gradient penalty\n",
    "            gp = gradient_penalty(tf.convert_to_tensor(y_target_train,dtype=tf.float32), fake, y_in_latent)\n",
    "            \n",
    "            # Add the gp to the original loss\n",
    "            d_loss = d_cost + gp * gp_weight\n",
    "        \n",
    "        # Get the gradients \n",
    "        d_gradient = tape.gradient(d_loss, discrr.trainable_variables)\n",
    "        \n",
    "        # Update the weights \n",
    "        discr_opt.apply_gradients(zip(d_gradient, discrr.trainable_variables))\n",
    "        \n",
    "    # Step 2: train the generator\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Generate fake results \n",
    "        generated_curves = genr(y_in_latent, training=True)\n",
    "        \n",
    "        # Get the discriminator logits for fake results\n",
    "        gen_logits = discrr([generated_curves,y_in_latent], training=True)\n",
    "        \n",
    "        # Calculate the generator loss\n",
    "        g_loss = generator_loss(gen_logits, generated_curves)\n",
    "        \n",
    "    # Record the values of the losses\n",
    "    record1[i] = record11(gen_logits, generated_curves)\n",
    "    record2[i] = record22(gen_logits, generated_curves)\n",
    "        \n",
    "    # Get the gradients \n",
    "    gen_gradient = tape.gradient(g_loss, genr.trainable_variables)\n",
    "    \n",
    "    # Update the weights \n",
    "    gen_opt.apply_gradients(zip(gen_gradient, genr.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure 3\n",
    "\n",
    "plt.plot(np.abs(record1),'k')\n",
    "plt.plot(np.abs(record2),'b')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend(['Adversarial error','MAE'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d798b",
   "metadata": {},
   "source": [
    "## CGAN vs CWGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df771d47",
   "metadata": {},
   "source": [
    "Run this code to generate figure 3 of the paper using pre-trained CGAN and CWGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef032f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "\n",
    "gen_CGAN = models.load_model('GAN_8metal_020_CGAN.h5')\n",
    "gen_CWGAN = models.load_model('GAN_8metal_020_v3g.h5')\n",
    "\n",
    "y_target_train_new = y_target_rand[train_index,:]\n",
    "\n",
    "# PCA\n",
    "\n",
    "cov_mat = np.matmul(np.transpose(y_target_train_new),y_target_train_new)/y_target_train_new.shape[0]\n",
    "\n",
    "[w,v] = np.linalg.eig(cov_mat)\n",
    "\n",
    "z = np.matmul(y_target_train_new,v[:,:7])\n",
    "z_CGAN = np.matmul(gen_CGAN(y_in_train),v[:,:7])\n",
    "z_CWGAN = np.matmul(gen_CWGAN(y_in_train),v[:,:7])\n",
    "\n",
    "# Check information PCA\n",
    "\n",
    "s = np.zeros(w.shape[0]) # Pre-allocate\n",
    "\n",
    "s0 = np.sum(w)\n",
    "\n",
    "for i in range(w.shape[0]):\n",
    "\n",
    "  si = np.sum(w[:(i+1)])\n",
    "  s[i] = si/s0\n",
    "\n",
    "print(s[1]) # First two elements, what we use for the figure\n",
    "print(s[7]) # First 8 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4a\n",
    "\n",
    "plt.scatter(z[:,0],z[:,1],color='darkgray')\n",
    "plt.scatter(z_CGAN[:,0],z_CGAN[:,1],color='forestgreen')\n",
    "plt.xlabel('$z_1$')\n",
    "plt.ylabel('$z_2$')\n",
    "plt.legend(['training set','generated'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4b\n",
    "\n",
    "plt.scatter(z[:,0],z[:,1],color='darkgray')\n",
    "plt.scatter(z_CWGAN[:,0],z_CWGAN[:,1],color='forestgreen')\n",
    "plt.xlabel('$z_1$')\n",
    "plt.ylabel('$z_2$')\n",
    "plt.legend(['training set','generated'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
